{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('roofpedia': conda)",
   "metadata": {
    "interpreter": {
     "hash": "47a60c9ace9e62894a2b4ad4c3afed5854ccc74f32a8a0a31ebc2392474e2ac4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Training + Prediction\n",
    "\n",
    "1. Data Augmentation\n",
    "2. Training\n",
    "3. Outputting validation scores such as mIOU or Lovasz\n",
    "4. outputting predicted masks or predicted geojson polygons directly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import collections\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "import torch.nn as nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Resize, CenterCrop, Normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from robosat.transforms import (\n",
    "    JointCompose,\n",
    "    JointTransform,\n",
    "    JointRandomHorizontalFlip,\n",
    "    JointRandomRotation,\n",
    "    ConvertImageMode,\n",
    "    ImageToTensor,\n",
    "    MaskToTensor,\n",
    ")\n",
    "\n",
    "from robosat.datasets import SlippyMapTilesConcatenation\n",
    "from robosat.metrics import Metrics\n",
    "from robosat.losses import CrossEntropyLoss2d, mIoULoss2d, FocalLoss2d, LovaszLoss2d\n",
    "from robosat.unet import UNet\n",
    "from robosat.utils import plot\n",
    "from robosat.config import load_config\n",
    "from robosat.log import Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parser(subparser):\n",
    "    parser = subparser.add_parser(\n",
    "        \"train\", help=\"trains model on dataset\", formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"path to model configuration file\")\n",
    "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"path to dataset configuration file\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, required=False, help=\"path to a model checkpoint (to retrain)\")\n",
    "    parser.add_argument(\"--resume\", type=bool, default=False, help=\"resume training or fine-tuning (if checkpoint)\")\n",
    "    parser.add_argument(\"--workers\", type=int, default=0, help=\"number of workers pre-processing images\")\n",
    "\n",
    "    parser.set_defaults(func=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_loaders(target_size, batch_size, dataset_path):\n",
    "    target_size = (target_size, target_size)\n",
    "    path = dataset_path\n",
    "    \n",
    "    # using imagenet mean and std for Normalization\n",
    "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = JointCompose(\n",
    "        [   \n",
    "            JointTransform(ConvertImageMode(\"RGB\"), ConvertImageMode(\"P\")),\n",
    "            JointTransform(Resize(target_size, Image.BILINEAR), Resize(target_size, Image.NEAREST)),\n",
    "            JointTransform(CenterCrop(target_size), CenterCrop(target_size)),\n",
    "            JointRandomHorizontalFlip(0.5),\n",
    "            JointRandomRotation(0.5, 90),\n",
    "            JointRandomRotation(0.5, 90),\n",
    "            JointRandomRotation(0.5, 90),\n",
    "            JointTransform(ImageToTensor(), MaskToTensor()),\n",
    "            JointTransform(Normalize(mean=mean, std=std), None),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = SlippyMapTilesConcatenation(\n",
    "        [os.path.join(path, \"training\", \"images\")], os.path.join(path, \"training\", \"labels\"), transform\n",
    "    )\n",
    "\n",
    "    val_dataset = SlippyMapTilesConcatenation(\n",
    "        [os.path.join(path, \"validation\", \"images\")], os.path.join(path, \"validation\", \"labels\"), transform\n",
    "    )\n",
    "\n",
    "    assert len(train_dataset) > 0, \"at least one tile in training dataset\"\n",
    "    assert len(val_dataset) > 0, \"at least one tile in validation dataset\"\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataset_loaders(256, 8, 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, num_classes, device, net, optimizer, criterion):\n",
    "    num_samples = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    # always two classes in our case\n",
    "    metrics = Metrics(range(num_classes))\n",
    "    # initialized model\n",
    "    net.train()\n",
    "    \n",
    "    # training loop\n",
    "    for images, masks, tiles in tqdm(loader, desc=\"Train\", unit=\"batch\", ascii=True):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        assert images.size()[2:] == masks.size()[1:], \"resolutions for images and masks are in sync\"\n",
    "\n",
    "        num_samples += int(images.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        for mask, output in zip(masks, outputs):\n",
    "            prediction = output.detach()\n",
    "            metrics.add(mask, prediction)\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_loss / num_samples,\n",
    "        \"miou\": metrics.get_miou(),\n",
    "        \"fg_iou\": metrics.get_fg_iou(),\n",
    "        \"mcc\": metrics.get_mcc(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, num_classes, device, net, criterion):\n",
    "    num_samples = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    metrics = Metrics(range(num_classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "\n",
    "        for images, masks, tiles in tqdm(loader, desc=\"Validate\", unit=\"batch\", ascii=True):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            assert images.size()[2:] == masks.size()[1:], \"resolutions for images and masks are in sync\"\n",
    "\n",
    "            num_samples += int(images.size(0))\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            for mask, output in zip(masks, outputs):\n",
    "                metrics.add(mask, output)\n",
    "\n",
    "        return {\n",
    "            \"loss\": running_loss / num_samples,\n",
    "            \"miou\": metrics.get_miou(),\n",
    "            \"fg_iou\": metrics.get_fg_iou(),\n",
    "            \"mcc\": metrics.get_mcc(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100%|##########| 84/84 [00:31<00:00,  2.64batch/s]\n",
      "Validate: 100%|##########| 4/4 [00:00<00:00,  6.72batch/s]\n",
      "Train: 100%|##########| 84/84 [00:29<00:00,  2.82batch/s]\n",
      "Validate: 100%|##########| 4/4 [00:00<00:00,  6.64batch/s]\n",
      "Train:  18%|#7        | 15/84 [00:05<00:25,  2.71batch/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9215d1d01e3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# log.log(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mtrain_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# log.log(\"Train loss: {:.4f}, mIoU: {:.3f}, {} IoU: {:.3f}, MCC: {:.3f}\".format(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-12623ae42c33>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader, num_classes, device, net, optimizer, criterion)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# weighted values for loss functions\n",
    "# add a helper to return weights seamlessly\n",
    "try:\n",
    "    weight = torch.Tensor([1.513212, 10.147043])\n",
    "except KeyError:\n",
    "    if model[\"opt\"][\"loss\"] in (\"CrossEntropy\", \"mIoU\", \"Focal\"):\n",
    "        sys.exit(\"Error: The loss function used, need dataset weights values\")\n",
    "\n",
    "# add in resume training if possible\n",
    "\n",
    "# loading Model\n",
    "net = UNet(num_classes)\n",
    "net = DataParallel(net)\n",
    "net = net.to(device)\n",
    "\n",
    "# define optimizer \n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# select loss function, just set a default, or try to experiment\n",
    "if loss_func == \"CrossEntropy\":\n",
    "    criterion = CrossEntropyLoss2d(weight=weight).to(device)\n",
    "elif loss_func == \"mIoU\":\n",
    "    criterion = mIoULoss2d(weight=weight).to(device)\n",
    "elif loss_func == \"Focal\":\n",
    "    criterion = FocalLoss2d(weight=weight).to(device)\n",
    "elif loss_func == \"Lovasz\":\n",
    "    criterion = LovaszLoss2d().to(device)\n",
    "else:\n",
    "    sys.exit(\"Error: Unknown Loss Function value !\")\n",
    "\n",
    "\n",
    "#loading data\n",
    "train_loader, val_loader = get_dataset_loaders(target_size, batch_size, dataset_path)\n",
    "\n",
    "# setup training logs\n",
    "# log = Log(checkpoint_path, \"log\")\n",
    "# log.log(\"--- Hyper Parameters on Dataset: {} ---\".format(dataset[\"common\"][\"dataset\"]))\n",
    "# log.log(\"Batch Size:\\t {}\".format(model[\"common\"][\"batch_size\"]))\n",
    "# log.log(\"Image Size:\\t {}\".format(model[\"common\"][\"image_size\"]))\n",
    "# log.log(\"Learning Rate:\\t {}\".format(model[\"opt\"][\"lr\"]))\n",
    "# log.log(\"Loss function:\\t {}\".format(model[\"opt\"][\"loss\"]))\n",
    "# if \"weight\" in locals():\n",
    "#     log.log(\"Weights :\\t {}\".format(dataset[\"weights\"][\"values\"]))\n",
    "# log.log(\"---\")\n",
    "\n",
    "history = collections.defaultdict(list)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    # log.log(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "    train_hist = train(train_loader, num_classes, device, net, optimizer, criterion)\n",
    "    \n",
    "    # log.log(\"Train loss: {:.4f}, mIoU: {:.3f}, {} IoU: {:.3f}, MCC: {:.3f}\".format(\n",
    "    #         train_hist[\"loss\"], train_hist[\"miou\"], target_type, train_hist[\"fg_iou\"], train_hist[\"mcc\"]))\n",
    "\n",
    "    for key, value in train_hist.items():\n",
    "        history[\"train \" + key].append(value)\n",
    "\n",
    "    # validate for each epoch\n",
    "    val_hist = validate(val_loader, num_classes, device, net, criterion)\n",
    "\n",
    "    # log.log(\"Validation loss: {:.4f}, mIoU: {:.3f}, {} IoU: {:.3f}, MCC: {:.3f}\".format(\n",
    "    #         val_hist[\"loss\"], val_hist[\"miou\"], target_type, val_hist[\"fg_iou\"], val_hist[\"mcc\"]))\n",
    "\n",
    "    for key, value in val_hist.items():\n",
    "        history[\"val \" + key].append(value)\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        # plotter use history values, no need for log\n",
    "        visual = \"history-{:05d}-of-{:05d}.png\".format(epoch + 1, num_epochs)\n",
    "        plot(os.path.join(checkpoint_path, visual), history)\n",
    "    \n",
    "    if (epoch+1)%1 == 0:\n",
    "        checkpoint = \"checkpoint-{:05d}-of-{:05d}.pth\".format(epoch + 1, num_epochs)\n",
    "        states = {\"epoch\": epoch + 1, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(states, os.path.join(checkpoint_path, checkpoint))"
   ]
  },
  {
   "source": [
    "### Predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading configs for training with argparse.\n",
    "# do we actually need this argparse?\n",
    "# maybe, but let's remove it for now\n",
    "# model = load_config(args.model)\n",
    "# dataset = load_config(args.dataset)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    sys.exit(\"Error: CUDA requested but not available\")\n",
    "\n",
    "# global vars\n",
    "# make it into a config file if needed for experiment\n",
    "num_classes = 2\n",
    "lr = 0.0005\n",
    "loss_func = \"Lovasz\"\n",
    "num_epochs = 10\n",
    "target_size = 256\n",
    "batch_size  = 8\n",
    "dataset_path = \"dataset\"\n",
    "checkpoint_path = \"checkpoint\"\n",
    "target_type = \"Solar\"\n",
    "# make dir for checkpoint\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from robosat.datasets import BufferedSlippyMapDirectory\n",
    "from robosat.unet import UNet\n",
    "from robosat.config import load_config\n",
    "from robosat.colors import continuous_palette_for_color\n",
    "from robosat.transforms import ConvertImageMode, ImageToTensor\n",
    "from robosat.colors import make_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional args for prediction\n",
    "checkpoint_name = \"Solar_Best.pth\"\n",
    "tile_size = 256\n",
    "weights = [1.513212, 10.147043]\n",
    "tiles_dir = \"Melbourne\"\n",
    "mask_dir = \"predicted_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Eval:  51%|#####     | 3840/7535 [02:41<02:35, 23.71batch/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-184ed3b157d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# don't track tensors with autograd during prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Eval\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\tqdm-4.59.0-py3.7.egg\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAL\\Roofpedia\\robosat\\datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer_tile_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverlap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAL\\Roofpedia\\robosat\\tiles.py\u001b[0m in \u001b[0;36mbuffer_tile_image\u001b[1;34m(tile, tiles, overlap, tile_size, nodata)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[0mtop_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjacent_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0mtop_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjacent_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[0mbottom_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjacent_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[0mbottom_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjacent_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAL\\Roofpedia\\robosat\\tiles.py\u001b[0m in \u001b[0;36madjacent_tile\u001b[1;34m(tile, dx, dy, tiles)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\roofpedia\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2903\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2904\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2905\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "chkpt = torch.load(os.path.join(checkpoint_path, checkpoint_name), map_location=device)\n",
    "\n",
    "# load device\n",
    "net = UNet(num_classes).to(device)\n",
    "net = nn.DataParallel(net)\n",
    "net.load_state_dict(chkpt[\"state_dict\"])\n",
    "net.eval()\n",
    "\n",
    "# preprocess and load\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "transform = Compose([ConvertImageMode(mode=\"RGB\"), ImageToTensor(), Normalize(mean=mean, std=std)])\n",
    "\n",
    "# tiles file, need to get it again, or do we really need it? why not just predict\n",
    "directory = BufferedSlippyMapDirectory(tiles_dir, transform=transform, size=tile_size)\n",
    "assert len(directory) > 0, \"at least one tile in dataset\"\n",
    "\n",
    "# loading data\n",
    "loader = DataLoader(directory, batch_size=1)\n",
    "\n",
    "# don't track tensors with autograd during prediction\n",
    "with torch.no_grad():\n",
    "    for images, tiles in tqdm(loader, desc=\"Eval\", unit=\"batch\", ascii=True):\n",
    "        images = images.to(device)\n",
    "        outputs = net(images)\n",
    "\n",
    "        # manually compute segmentation mask class probabilities per pixel\n",
    "        probs = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()\n",
    "\n",
    "        for tile, prob in zip(tiles, probs):\n",
    "            x, y, z = list(map(int, tile))\n",
    "\n",
    "            prob = directory.unbuffer(prob)\n",
    "            mask = np.argmax(prob, axis=0)\n",
    "            mask = mask*200\n",
    "            mask = mask.astype(np.uint8)\n",
    "\n",
    "            palette = make_palette(\"dark\", \"light\")\n",
    "            out = Image.fromarray(mask, mode=\"P\")\n",
    "            out.putpalette(palette)\n",
    "\n",
    "            os.makedirs(os.path.join(mask_dir, str(z), str(x)), exist_ok=True)\n",
    "            path = os.path.join(mask_dir, str(z), str(x), str(y) + \".png\")\n",
    "            out.save(path, optimize=True)"
   ]
  },
  {
   "source": [
    "### Post-Processing\n",
    "\n",
    "1. From predicted masks to predicted geojson polygons for either Solar or Green\n",
    "\n",
    "2. Loading Building Polygons\n",
    "\n",
    "3. Comparative comparisons between Solar/Green with building polygons\n",
    "\n",
    "4. Outputing highlighted building polygons\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robosat.tiles import tiles_from_slippy_map\n",
    "\n",
    "from robosat.features.parking import ParkingHandler\n",
    "\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = toml.load('config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mask_dir': 'predicted_masks',\n",
       " 'common': {'dataset': '/tmp/slippy-map-dir/',\n",
       "  'classes': ['background', 'parking'],\n",
       "  'colors': ['denim', 'orange']},\n",
       " 'weights': {'values': [1.6248, 5.762827]},\n",
       " 'commo1n': {'cuda': True,\n",
       "  'batch_size': 2,\n",
       "  'image_size': 512,\n",
       "  'checkpoint': '/tmp/pth/'},\n",
       " 'opt': {'epochs': 10, 'lr': 0.0001, 'loss': 'Lovasz'}}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'predicted_masks'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "config['mask_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "handler = ParkingHandler()\n",
    "\n",
    "tiles = list(tiles_from_slippy_map(mask_dir))\n",
    "\n",
    "for tile, path in tqdm(tiles, ascii=True, unit=\"mask\"):\n",
    "    image = np.array(Image.open(path).convert(\"P\"), dtype=np.uint8)\n",
    "    mask = (image == 1).astype(np.uint8)\n",
    "    handler.apply(tile, mask)\n",
    "\n",
    "# output feature collection\n",
    "handler.save(os.path.join(output_path, \"feature.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_feature(output_folder):\n",
    "\n",
    "    handler = ParkingHandler()\n",
    "    \n",
    "    tiles = list(tiles_from_slippy_map(mask_dir))\n",
    "\n",
    "    for tile, path in tqdm(tiles, ascii=True, unit=\"mask\"):\n",
    "        image = np.array(Image.open(path).convert(\"P\"), dtype=np.uint8)\n",
    "        mask = (image == 1).astype(np.uint8)\n",
    "        handler.apply(tile, mask)\n",
    "\n",
    "    # output feature collection\n",
    "    handler.save(os.path.join(output_path, \"feature.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}